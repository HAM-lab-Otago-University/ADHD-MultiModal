{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import libs\n",
    "import numpy as np\n",
    "import plotly\n",
    "from plotly.subplots import make_subplots\n",
    "import os\n",
    "import matplotlib.pyplot as plt #to enable plotting within notebook\n",
    "import matplotlib\n",
    "from nilearn import image as nimg\n",
    "from nilearn import plotting \n",
    "from bids.layout import BIDSLayout\n",
    "import bids\n",
    "from matplotlib.pyplot import figure\n",
    "import mpld3\n",
    "import pandas as pd\n",
    "from pathlib import Path   \n",
    "import nibabel as nb \n",
    "import plotly.express as px\n",
    "from nilearn.datasets import fetch_icbm152_brain_gm_mask\n",
    "from nilearn.plotting import plot_roi\n",
    "from nilearn.glm.first_level import make_first_level_design_matrix, run_glm\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from nilearn.glm.contrasts import compute_contrast\n",
    "from nilearn.plotting import plot_contrast_matrix\n",
    "from nilearn.reporting import make_glm_report\n",
    "from nilearn.interfaces.bids import save_glm_to_bids\n",
    "from ordered_set import OrderedSet\n",
    "from nilearn.glm.first_level import first_level_from_bids\n",
    "import pickle\n",
    "import os.path\n",
    "import pathlib\n",
    "import gc\n",
    "from nilearn import image\n",
    "from templateflow import api as tflow\n",
    "from nilearn.maskers import NiftiMasker\n",
    "import traceback\n",
    "import logging\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "import nilearn as nl\n",
    "from nilearn.interfaces.fmriprep import load_confounds\n",
    "import json\n",
    "from nilearn import surface\n",
    "import cv2\n",
    "\n",
    "import nibabel as nb\n",
    "from pathlib import Path              # Combine path elements with /\n",
    "from pprint import pprint             # Pretty-printing\n",
    "import numpy as np                    # Numeric Python\n",
    "from matplotlib import pyplot as plt  # Matlab-ish plotting commands\n",
    "from nilearn import plotting as nlp   # Nice neuroimage plotting\n",
    "import transforms3d                   # Work with affine algebra\n",
    "from scipy import ndimage as ndi      # Operate on N-dimensional images\n",
    "import nibabel.testing                # For fetching test data\n",
    "import hcp_utils as hcp\n",
    "##import libs for glm and plots\n",
    "import nilearn\n",
    "import warnings\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.interpolate import CubicSpline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The working directory has been changed!\n"
     ]
    }
   ],
   "source": [
    "##Set path to the data folder\n",
    "os.chdir('/media/hcs-sci-psy-narun/OpenNEURO_adhd/ds002424-download/')\n",
    "print(\"The working directory has been changed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set path & get layout object for later use:\n",
    "data_dir = '/media/hcs-sci-psy-narun/OpenNEURO_adhd/ds002424-download/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set variable for each dir task = WM\n",
    "\n",
    "# task_label = ['SLD', 'SSI', 'SSD', 'SLI', 'VLD', 'VSI', 'VSD', 'VLI']\n",
    "space_label = \"MNI152NLin2009cAsym\"\n",
    "derivatives_folder = \"derivatives/fmriprep\"\n",
    "# direction = ['LR', 'RL']\n",
    "#contrast_type = 'story-math'\n",
    "ses = 'T1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "subject_ID_df  = pd.read_csv(data_dir + \"valid_subj_list.csv\", dtype=str, index_col=0)\n",
    "subject_ID = list(subject_ID_df.iloc[:,0])\n",
    "subject_ID = [str(x[4:]) for x in subject_ID]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subject_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function: extract desired confounds\n",
    "def manage_confounds(original_confs):\n",
    "\n",
    "\n",
    "       cosine = original_confs.filter(regex='^cosine',axis=1)\n",
    "       cosine_regs = list(cosine.columns)\n",
    "       non_steady = original_confs.filter(regex='^non_steady_state',axis=1)\n",
    "       a_comp_cor = ['c_comp_cor_00', 'c_comp_cor_01', 'c_comp_cor_02', 'c_comp_cor_03', 'c_comp_cor_04',\n",
    "                      'w_comp_cor_00', 'w_comp_cor_01', 'w_comp_cor_02', 'w_comp_cor_03', 'w_comp_cor_04']\n",
    "       movement_regs = ['rot_x', 'rot_x_derivative1','rot_y','rot_y_derivative1','rot_z','rot_z_derivative1',\n",
    "                      'trans_x', 'trans_x_derivative1','trans_y', 'trans_y_derivative1','trans_z', 'trans_z_derivative1']\n",
    "       fd = ['framewise_displacement', 'std_dvars']\n",
    "       desired_confs = a_comp_cor + cosine_regs + movement_regs + fd\n",
    "\n",
    "\n",
    "       if ~set(desired_confs).issubset(original_confs.columns):\n",
    "              all_confs = original_confs.columns.values.tolist()\n",
    "              mising_confs = list(set(desired_confs) - set(all_confs))\n",
    "              final_confounds = list(OrderedSet(desired_confs) - OrderedSet(mising_confs))\n",
    "       else:\n",
    "              final_confounds = desired_confs\n",
    "       \n",
    "       #get dummy scan rows     \n",
    "       temp = original_confs.isna().any(axis=1)\n",
    "       na_ind = list(temp[temp].index)\n",
    "       dummy = list(np.where(non_steady == 1)[0])\n",
    "       dummy_scans = list(set(na_ind+dummy))\n",
    "       confs_final = original_confs.drop(dummy_scans,axis=0,inplace=False)\n",
    "       confs_final = confs_final.reset_index(drop=True)\n",
    "\n",
    "       confs_final = confs_final.loc[:,final_confounds]\n",
    "       confs_final.insert(confs_final.shape[1],'linear_trend', range(len(dummy_scans), original_confs.shape[0]))\n",
    "\n",
    "       return confs_final, dummy_scans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function: create & save plots\n",
    "def manage_plots(unparced_z, unparced_eff , sub_id, contrast_t, task_label, output_dir):\n",
    "     surface_zs = nl.plotting.view_surf(hcp.mesh.inflated,\n",
    "     hcp.cortex_data(unparced_z), \n",
    "     bg_map=hcp.mesh.sulc, cmap='bwr')\n",
    "     surface_zs.save_as_html(output_dir + '/sub-%s_ses-%s_task-%s_contrast-%s_z_parcelated_plot.html' %(sub_id, ses, task_label, contrast_t))\n",
    "     surface_eff = nl.plotting.view_surf(hcp.mesh.inflated,\n",
    "     hcp.cortex_data(unparced_eff), \n",
    "     bg_map=hcp.mesh.sulc, cmap='bwr')\n",
    "     surface_eff.save_as_html(output_dir + '/sub-%s_ses-%s_task-%s_contrast-%s_effect_parcelated_plot.html' %(sub_id, ses, task_label, contrast_t))\n",
    "     plt.close('all')\n",
    "     gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function: prepare events file:\n",
    "def prepare_events(ind_events,dummy_scans, t_r):\n",
    "    n_dummy = len(dummy_scans)\n",
    "    if n_dummy != 0:\n",
    "        #print(n_dummy)\n",
    "        new_zero = n_dummy * t_r\n",
    "        ind_events['onset'] = ind_events['onset'] - new_zero\n",
    "        invalid_row_ind = np.asarray(ind_events['onset'] < 0).nonzero()\n",
    "        if len(invalid_row_ind[0]) != 0:\n",
    "            del_row = []\n",
    "            for i in invalid_row_ind[0]:\n",
    "                # print(i)\n",
    "                ii=i\n",
    "                new_dur = ind_events.at[ii,'onset']+ind_events.at[ii,'duration']\n",
    "                if new_dur > 0:\n",
    "                    ind_events.at[ii,'onset'] = 0 \n",
    "                    ind_events.at[ii,'duration'] = new_dur\n",
    "                else:\n",
    "                    del_row.append(ii)\n",
    "            ind_events = ind_events.drop(del_row)  \n",
    "    \n",
    "    return  ind_events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_block(ind_events):\n",
    "\n",
    "    ind_events = ind_events[['onset', 'duration', 'trial_type']]\n",
    "    for i in range(len(ind_events) - 2):\n",
    "        if ind_events.at[i, 'trial_type'] == 'no_response':\n",
    "            ind_events.at[i, 'trial_type'] = ind_events.at[i + 3, 'trial_type']\n",
    "    # print(ind_events)\n",
    "    # Identify block starts and ends\n",
    "    ind_events['next_trial_type'] = ind_events['trial_type'].shift(-1)\n",
    "    ind_events['block_end'] = ind_events['trial_type'] != ind_events['next_trial_type']\n",
    "    ind_events['block_start'] = ind_events['block_end'].shift(1, fill_value=True)\n",
    "\n",
    "    # Compute block onsets and ends\n",
    "    block_starts = ind_events[ind_events['block_start']].copy()\n",
    "    block_ends = ind_events[ind_events['block_end']].copy()\n",
    "\n",
    "    # Ensure the same index for merging\n",
    "    block_starts.index = range(len(block_starts))\n",
    "    block_ends.index = range(len(block_ends))\n",
    "\n",
    "    # Merge to get block start and end times\n",
    "    blocks = block_starts.copy()\n",
    "    blocks['block_onset'] = block_starts['onset']\n",
    "    blocks['block_end'] = block_ends['onset'] + block_ends['duration']\n",
    "\n",
    "    # Compute the block duration\n",
    "    blocks['block_duration'] = blocks['block_end'] - blocks['block_onset']\n",
    "\n",
    "    # Select relevant columns\n",
    "    block_df = blocks[['block_onset', 'block_duration', 'trial_type']]\n",
    "    block_df.columns = ['onset', 'duration', 'trial_type']\n",
    "\n",
    "    return block_df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function: Reshape, Parcellate \n",
    "def R_P (stat_map):\n",
    "    ##parcelate full\n",
    "    col = np.ones(len(stat_map))\n",
    "    map_2d=np.stack((stat_map,col),axis=1)\n",
    "    map_parc_data = np.transpose(map_2d)\n",
    "\n",
    "    parced_map = hcp.parcellate(map_parc_data, hcp.mmp)\n",
    "    unparced_map = hcp.unparcellate(parced_map[0], hcp.mmp)\n",
    "\n",
    "    return parced_map, unparced_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function:  convert to cifti \n",
    "def convert_to_cifti (stat_map, axes, t_r):\n",
    "    ##prepare save as cifti\n",
    "    #map = np.reshape(stat_map, (-1, stat_map.shape[0]))\n",
    "    map = stat_map\n",
    "    #save contrasts as cifti full\n",
    "    scalar_axis_map = nb.cifti2.SeriesAxis(start=0, step=t_r, size=map.shape[0])  # Takes a list of names, one per row\n",
    "    map_header = nb.Cifti2Header.from_axes([scalar_axis_map, axes[1]])\n",
    "    map_img = nb.Cifti2Image(map, header = map_header, ) \n",
    "                \n",
    "    return map_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function: flag outliers:\n",
    "def flag_outliers(confounds):\n",
    "    fd_threshold = 0.5  #from xcp-d #.3 in thomas paper\n",
    "    std_DVARS_threshold = 1.5  #from nilearn load_conf doc\n",
    "    #DVARS_threshold = 50 #from thomas paper\n",
    "    outlier_index = []  # just vols with high fd\n",
    "    censor_index = []  #all vols to be removed including high motion and surroundings\n",
    "    for ind , vol in enumerate(confounds['framewise_displacement']):\n",
    "        if vol > fd_threshold or confounds['std_dvars'][ind] > std_DVARS_threshold: \n",
    "            outlier_index.append(ind)\n",
    "            censor_index.extend([ind-2, ind-1, ind, ind+1])\n",
    "    for item in outlier_index:\n",
    "        if item+6 in censor_index:\n",
    "            censor_index.extend(list(range(item+2 , item+6)))\n",
    "            \n",
    "    censor_index = [x for x in censor_index if x > -1 and x < confounds.shape[0]] \n",
    "    censor_set = set(censor_index)\n",
    "    censor_index = list(censor_set)\n",
    "    censor_index.sort()\n",
    "    data_ind = list(confounds.index)\n",
    "    censored = list(OrderedSet(data_ind) - OrderedSet(censor_index))\n",
    "    censored_mask = np.asarray(censored)  # contains vols left after removing high motion and the surroundings\n",
    "    bi_mask = list(OrderedSet(data_ind) - OrderedSet(outlier_index))\n",
    "    bi_mask = [False if s in outlier_index else True for s in data_ind]\n",
    "    bi_mask = np.asarray(bi_mask) \n",
    "\n",
    "    return outlier_index, censor_index, censored_mask, bi_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_conn(parced_sig, path, sub_label, task_label , cen):\n",
    "    headers = list(hcp.mmp.labels.values())[1:]\n",
    "    ##plot nilearn parced correlation matrix\n",
    "    correlation_measure = ConnectivityMeasure(kind=\"correlation\")\n",
    "    correlation_matrix = correlation_measure.fit_transform([parced_sig])[0]\n",
    "\n",
    "    pd.DataFrame(correlation_matrix, columns = headers).to_csv(path +\n",
    "                                    '/sub-%s_ses-%s_task-%s_space-fsLR_atlas-Glasser_desc-%s_measure-pearsoncorrelation_conmat.tsv' %(sub_label, ses, task_label, cen), sep='\\t')\n",
    "    # plot connectivity matrix\n",
    "    conmat_fig = plotting.plot_matrix(\n",
    "        correlation_matrix,\n",
    "        figure=(10, 8),\n",
    "        labels=headers, \n",
    "        vmax=1,\n",
    "        vmin=-1,\n",
    "        #title=\"Confounds\",\n",
    "        #reorder=True,\n",
    "    )\n",
    "    conmat_fig.figure.savefig(path + '/sub-%s_ses-%s_task-%s_space-fsLR_atlas-Glasser_desc-%s_measure-pearsoncorrelation_conmat.png' %(sub_label, ses, task_label, cen))\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def despiking(signal):\n",
    "    cen_desp = signal.copy()\n",
    "    spike_threshold = 3.0\n",
    "    for i in range(cen_desp.shape[1]):  # loop over columns\n",
    "        vertex_data = cen_desp[:, i]\n",
    "        mean_value = np.mean(vertex_data)\n",
    "        spike_indices = np.where(np.abs(vertex_data - mean_value) > spike_threshold)[0]\n",
    "        \n",
    "        # Calculate mean without spike values\n",
    "        non_spike_indices = np.setdiff1d(np.arange(len(vertex_data)), spike_indices)\n",
    "        if len(non_spike_indices) > 0:\n",
    "            replacement_value = np.mean(vertex_data[non_spike_indices])\n",
    "        else:\n",
    "            replacement_value = mean_value  # fallback if all are spikes\n",
    "        \n",
    "        vertex_data[spike_indices] = replacement_value\n",
    "        cen_desp[:, i] = vertex_data  # reassign modified column\n",
    "\n",
    "    return cen_desp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function: run GLM and extract contrasts\n",
    "def surf_contrast(iter, sub_label, data_dir, output_dir):\n",
    "    try:\n",
    "        warnings.filterwarnings(action='ignore')\n",
    "        # map_dict = {1: \"oneback\", 2:\"twoback\"}\n",
    "        path = (output_dir+'sub-{}'.format(sub_label))\n",
    "        #print(path)\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "        #plt.ioff()\n",
    "\n",
    "        # set up lists for averaging across direction\n",
    "        cen_res_list = []\n",
    "        uncen_res_list = []\n",
    "        censored_vols = []\n",
    "        outlier_vols = []\n",
    "        half_vols = []\n",
    "        print('start:'+sub_label)\n",
    "\n",
    "        task_labels = ['VLD', 'VSI', 'VSD', 'VLI']\n",
    "        # task_labels = ['SLD', 'SSI', 'SSD', 'SLI']\n",
    "        for task_label in task_labels:\n",
    "\n",
    "            json_file = data_dir + 'derivatives/fmriprep/sub-%s/ses-T1/func/sub-%s_ses-T1_task-%s_space-fsLR_den-91k_bold.json' %(sub_label,sub_label, task_label)\n",
    "            if os.path.isfile(json_file):\n",
    "                with open(json_file, 'r') as f:\n",
    "                    t_r = json.load(f)['RepetitionTime']\n",
    "\n",
    "                fmriprep_bold = (data_dir + 'derivatives/fmriprep/sub-%s/ses-T1/func/sub-%s_ses-T1_task-%s_space-fsLR_den-91k_bold.dtseries.nii' %(sub_label, sub_label, task_label))\n",
    "                eve_dir = (data_dir + 'sub-%s/ses-T1/func/sub-%s_ses-T1_task-%s_events.tsv' %(sub_label, sub_label, task_label))\n",
    "                conf_dir = (data_dir + 'derivatives/fmriprep/sub-%s/ses-T1/func/sub-%s_ses-T1_task-%s_desc-confounds_timeseries.tsv' %(sub_label, sub_label, task_label))\n",
    "\n",
    "                events = pd.read_csv(eve_dir , sep='\\t')\n",
    "                confounds = pd.read_csv(conf_dir, sep='\\t')\n",
    "                #func_mni_img = nimg.load_img(fmriprep_bold)\n",
    "\n",
    "                # call confounds function:\n",
    "                [confounds_final, dummy_scans] = manage_confounds(confounds)\n",
    "\n",
    "                # confounds_final = confounds_final.reset_index(drop=True)\n",
    "\n",
    "                # calculate frametimes using confounds file n_rows and T_R\n",
    "                frame_times = (np.arange(confounds_final.shape[0]) * t_r)# + (t_r/2)\n",
    "\n",
    "                # call prepare event file function\n",
    "                ind_events = events.copy()\n",
    "                # fix events names\n",
    "                ind_events = ind_events.replace(regex=['1-'],value='one')\n",
    "                ind_events = ind_events.replace(regex=['2-'],value='two')\n",
    "                # ind_events['trial_type']= ind_events['trial_type'].map(map_dict)\n",
    "                # print(ind_events.loc[:,'trial_type'])\n",
    "                ind_events = to_block(ind_events)\n",
    "\n",
    "                ind_events = prepare_events(ind_events, dummy_scans, t_r)\n",
    "                # print(ind_events.loc[:,'trial_type'])\n",
    "                # ind_events = ind_events.reset_index(drop=True)\n",
    "\n",
    "                # save events file\n",
    "                ind_events.to_csv(output_dir + \n",
    "                                'sub-%s/sub-%s_ses-%s_task-%s_events.tsv' %(sub_label, sub_label, ses, task_label), sep='\\t')\n",
    "                confounds_final.to_csv(output_dir + \n",
    "                                'sub-%s/sub-%s_ses-%s_task-%s_confs.tsv' %(sub_label, sub_label, ses, task_label), sep='\\t')\n",
    "                \n",
    "                # identify and extract outliers\n",
    "                outliers, censor_ind, censored_mask, bi_mask = flag_outliers(confounds_final)\n",
    "                # print(censor_ind)\n",
    "                censored_vols.append(len(censor_ind))\n",
    "                outlier_vols.append(len(outliers))\n",
    "                half_vols.append(confounds_final.shape[0]/2)\n",
    "                pd.DataFrame(bi_mask).to_csv(path +\n",
    "                                                '/sub-%s_ses-%s_task-%s_outliers.tsv' %(sub_label, ses, task_label), sep='\\t')\n",
    "                \n",
    "                confounds_final = confounds_final.drop(['framewise_displacement', 'std_dvars'],axis=1,inplace=False) # drop fd and dvars cols before creating design mat\n",
    "\n",
    "                # create design mat\n",
    "                design_matrix = make_first_level_design_matrix(\n",
    "                                                                frame_times, events=ind_events,\n",
    "                                                                hrf_model='spm', drift_model= None,\n",
    "                                                                add_regs=confounds_final)\n",
    "\n",
    "                # for test :\n",
    "                unt_des_mat = design_matrix.copy()\n",
    "                # remove all flagged vols/ motion outliers from design mat for censored\n",
    "                cenlist = []\n",
    "                desmat_ind = design_matrix.index.tolist()\n",
    "                for i , ind in enumerate(desmat_ind):\n",
    "                    if i in censor_ind:\n",
    "                        cenlist.append(ind)\n",
    "                cen_des_mat = design_matrix.drop(cenlist,axis=0,inplace=False)\n",
    "\n",
    "                # add outliers to design mat for uncensored\n",
    "                for n , ind in enumerate(outliers):\n",
    "                    col_name = 'outlier%s' %str(n)\n",
    "                    out_vals = np.zeros(len(confounds_final))\n",
    "                    out_vals[ind] = 1\n",
    "                    design_matrix.insert(design_matrix.shape[1], col_name , out_vals)\n",
    "\n",
    "                design_plot = plotting.plot_design_matrix(design_matrix)\n",
    "                design_plot.figure.savefig(path + '/sub-%s_ses-%s_task-%s_design.svg' %(sub_label, ses, task_label))\n",
    "                design_plot2 = plotting.plot_design_matrix(cen_des_mat)\n",
    "                design_plot2.figure.savefig(path + '/sub-%s_ses-%s_task-%s_censored-design.svg' %(sub_label, ses, task_label))\n",
    "                cen_des_mat.to_csv(path + '/sub-%s_ses-%s_task-%s_censored-design.tsv' %(sub_label, ses, task_label), sep='\\t')\n",
    "                design_matrix.to_csv(path + '/sub-%s_ses-%s_task-%s_design.tsv' %(sub_label, ses, task_label), sep='\\t')\n",
    "\n",
    "                del confounds, events\n",
    "\n",
    "                # load image file and remove dummy scans:\n",
    "                cifti = nb.load(fmriprep_bold)\n",
    "                cifti_data = cifti.get_fdata()\n",
    "                cifti_hdr = cifti.header\n",
    "                nifti_hdr = cifti.nifti_header\n",
    "                axes = [cifti_hdr.get_axis(i) for i in range(cifti.ndim)]\n",
    "                cifti_data = cifti_data[len(dummy_scans):,:]     #uncensored\n",
    "                #standardize:\n",
    "                std_uncen = nl.signal.clean(cifti_data, detrend=False, standardize='zscore_sample', confounds=None, standardize_confounds=False,\n",
    "                    filter=False, low_pass=None, high_pass=None, t_r=t_r, ensure_finite=False)\n",
    "                #despike:\n",
    "                std_desp = despiking(std_uncen)\n",
    "                #scrub:\n",
    "                std_desp_cen = np.delete(std_desp, censor_ind, axis=0)   #censored - despiked\n",
    "\n",
    "                del cifti\n",
    "                \n",
    "                # fit the model \n",
    "                labels, estimates = run_glm(std_desp_cen, cen_des_mat.values, noise_model='ols')  # to censored signal\n",
    "                res_sig_cen = estimates[0.0].residuals\n",
    "\n",
    "                labels2, estimates2 = run_glm(std_desp, design_matrix.values, noise_model='ols')  # to uncensored signal\n",
    "                res_sig_uncen = estimates2[0.0].residuals\n",
    "                fig1, ax1 = plt.subplots()\n",
    "\n",
    "                # Plot the data on the axis\n",
    "\n",
    "                # ax1.plot(cifti_data[:, 4], color='yellow')\n",
    "                ax1.plot(std_uncen[:, 4], color='purple')\n",
    "                ax1.plot(std_desp[:, 4], color='black')\n",
    "                ax1.plot(std_desp_cen[:, 4], color='pink')\n",
    "\n",
    "                # Save the figure\n",
    "                fig1.savefig(path + '/timeseries.png')\n",
    "                fig2, ax2 = plt.subplots()\n",
    "                ax2.plot(res_sig_uncen[:, 4], color='red')\n",
    "                # ax2.plot(res_sig_cen[:, 4], color='blue')\n",
    "                # ax2.plot(interplotated_res[:, 4], color='green')\n",
    "                # ax2.plot(filtered_interplotated_res[:, 0], color='orange')  # Uncomment if needed\n",
    "                # Save the figure\n",
    "                fig2.savefig(path + '/res_timeseries.png')\n",
    "                # labels3, estimates3 = run_glm(cen_desp_cifti_data, cen_des_mat.values, noise_model='ols')  # to uncensored signal\n",
    "                # res_sig_unt = estimates3[0.0].residuals\n",
    "\n",
    "                del cifti_data, design_matrix, confounds_final, std_uncen, std_desp, std_desp_cen\n",
    "\n",
    "                # save residuals\n",
    "                uncen_res_img = convert_to_cifti(res_sig_uncen, axes, t_r)\n",
    "                uncen_res_img.to_filename(path + '/sub-%s_ses-%s_task-%s_space-fsLR_den-91k_desc-uncensoredDespiked_residuals.dtseries.nii' %(sub_label, ses, task_label))\n",
    "                cen_res_img = convert_to_cifti(res_sig_cen, axes, t_r)\n",
    "                cen_res_img.to_filename(path + '/sub-%s_ses-%s_task-%s_space-fsLR_den-91k_desc-censoredDespiked_residuals.dtseries.nii' %(sub_label, ses, task_label))\n",
    "\n",
    "                # interpolate uncensored residual\n",
    "                interplotated_res = res_sig_uncen\n",
    "                remained_vol = frame_times[bi_mask]\n",
    "                remained_x = interplotated_res[bi_mask, :]\n",
    "                cubic_spline_fitter = CubicSpline(remained_vol, remained_x)\n",
    "                volumes_interpolated = cubic_spline_fitter(frame_times)\n",
    "                interplotated_res[~bi_mask, :] = volumes_interpolated[~bi_mask, :]\n",
    "                # inter_res_img = convert_to_cifti(interplotated_res, axes, t_r)\n",
    "                # inter_res_img.to_filename(path + '/sub-%s_ses-%s_task-%s_dir-%s_space-fsLR_den-91k_desc-interpolatedDenoised_residuals.dtseries.nii' %(sub_label, ses, task_label, dir[dir_ind]))\n",
    "\n",
    "                # perform filtering\n",
    "                # sr = 1.0 / t_r\n",
    "                #filtered_interplotated_res = nl.signal.butterworth(interplotated_res, sr , low_pass=0.08, high_pass=0.01, order=2)\n",
    "\n",
    "                # parcellate:\n",
    "                parced_cen_res = hcp.parcellate(res_sig_cen, hcp.mmp)\n",
    "                parced_uncen_res = hcp.parcellate(res_sig_uncen, hcp.mmp)\n",
    "                parced_interplotated_res = hcp.parcellate(interplotated_res, hcp.mmp)\n",
    "                ## read glasser header from hcp\n",
    "                headers = list(hcp.mmp.labels.values())[1:]\n",
    "                # save parcellation\n",
    "                # pres_img = convert_to_cifti(parced_res, axes, t_r)\n",
    "                # pres_img.to_filename(path + '/sub-%s_task-%s_dir-%s_space-fsLR_den-91k_desc-uncensoredDenoisedParcelated_residuals.dtseries.nii' %(sub_label, task_label, dir[dir_ind]))\n",
    "                pd.DataFrame(parced_uncen_res, columns = headers).to_csv(path +\n",
    "                                    '/sub-%s_ses-%s_task-%s_space-fsLR_atlas-Glasser_desc-uncensoredDespiked_parcelations.tsv' %(sub_label, ses, task_label), sep='\\t')\n",
    "                pd.DataFrame(parced_cen_res, columns = headers).to_csv(path +\n",
    "                                    '/sub-%s_ses-%s_task-%s_space-fsLR_atlas-Glasser_desc-censoredDespiked_parcelations.tsv' %(sub_label, ses, task_label), sep='\\t')\n",
    "                pd.DataFrame(parced_interplotated_res, columns = headers).to_csv(path +\n",
    "                                    '/sub-%s_ses-%s_task-%s_space-fsLR_atlas-Glasser_desc-censoredDespikedInterp_parcelations.tsv' %(sub_label, ses, task_label), sep='\\t')\n",
    "                # connectivity:\n",
    "                compute_conn(parced_cen_res, path, sub_label, task_label, \"censored\")\n",
    "                compute_conn(parced_uncen_res, path, sub_label, task_label , \"uncensored\")\n",
    "                compute_conn(parced_interplotated_res, path, sub_label, task_label , \"censoredInterp\")\n",
    "                # keep for sum of runs\n",
    "                cen_res_list.append(res_sig_cen)\n",
    "                uncen_res_list.append(res_sig_uncen)\n",
    "                del  cen_res_img, parced_cen_res , res_sig_uncen , uncen_res_img , res_sig_cen, parced_uncen_res\n",
    "                plt.close('all')\n",
    "\n",
    "        plt.close('all')\n",
    "\n",
    "        dirM = 'verbal' # verbalspatial\n",
    "        # # combine directions and calculate mean \n",
    "        full_cen_res = np.concatenate(cen_res_list, axis=0)\n",
    "        parced_full_cen_res = hcp.parcellate(full_cen_res, hcp.mmp)\n",
    "        pd.DataFrame(parced_full_cen_res, columns = headers).to_csv(path +\n",
    "                                            '/sub-%s_task-%s_space-fsLR_atlas-Glasser_desc-censoredDespiked_parcelations.tsv' %(sub_label, dirM), sep='\\t')\n",
    "        compute_conn(parced_full_cen_res, path, sub_label, dirM, \"censored\")\n",
    "        \n",
    "        full_uncen_res = np.concatenate(uncen_res_list, axis=0)\n",
    "        parced_full_uncen_res = hcp.parcellate(full_uncen_res, hcp.mmp)\n",
    "        pd.DataFrame(parced_full_uncen_res, columns = headers).to_csv(path +\n",
    "                                            '/sub-%s_task-%s_space-fsLR_atlas-Glasser_desc-uncensoredDespiked_parcelations.tsv' %(sub_label, dirM), sep='\\t')\n",
    "        compute_conn(parced_full_uncen_res, path, sub_label, dirM, \"uncensored\")\n",
    "\n",
    "        censored_vols.append(sum(censored_vols))\n",
    "        outlier_vols.append(sum(outlier_vols))\n",
    "        half_vols.append(sum(half_vols))\n",
    "        \n",
    "        pd.DataFrame({'round': [dirM ], 'del vols': censored_vols[-1], 'N of vols needed': half_vols[-1], 'high motion vols': outlier_vols[-1]}).to_csv(path + \n",
    "                            '/sub-%s_ses-%s_task-%s_scrubbing_log.tsv' %(sub_label, ses, dirM), sep='\\t')\n",
    "        \n",
    "        plt.close('all')\n",
    "        print('done:'+sub_label)\n",
    "        \n",
    "        gc.collect()\n",
    "    except Exception as e:\n",
    "        #error.append(sub_label)\n",
    "        logging.error(traceback.format_exc())\n",
    "    # except ValueError:\n",
    "    #     print(model.subject_label +'raised value error')\n",
    "    # except TypeError:\n",
    "    #     print(model.subject_label +'raised type error')\n",
    "    # except MemoryError:\n",
    "    #     print(model.subject_label +'raised memory error')    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run parallel\n",
    "import warnings\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from joblib import parallel_backend\n",
    "\n",
    "from joblib import Memory\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "#plt.ioff()\n",
    "output_dir = data_dir + \"derivatives/nilearn_glm/tconnectivity_WM/scrubbed-despiked/\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "Parallel(n_jobs=40)(delayed(surf_contrast)(iter, subject_label, data_dir, output_dir) for iter, subject_label in enumerate(subject_ID))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nilearn_py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
